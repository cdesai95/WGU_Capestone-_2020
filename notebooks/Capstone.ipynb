{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# data in/out & eda\n",
    "import pandas as pd \n",
    "import pandas_profiling\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in Dayton's power consumption data\n",
    "df = pd.read_csv('/home/jovyan/Capstone_Data_Files/Capstone_Dayton_Enrgy_Data/DAYTON_hourly.csv')\n",
    "\n",
    "# sort by date & time\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df.sort_values(by=['Datetime'], axis=0, ascending=True, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# renaming the target variable columns\n",
    "df.rename(columns={'DAYTON_MW':'EnergyDemand_in_MW'}, inplace=True)\n",
    "\n",
    "# display the first couple of rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate to remove redundancies, only keeping the last measurement per datetime\n",
    "df.drop_duplicates(subset='Datetime', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and Fill Missing DateTime Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have a continuous dataset\n",
    "df = df.set_index('Datetime')\n",
    "print(f'df.index.freq is set to: {df.index.freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, datetime index's frequency set to None is an indication that there are some missing data points somewhere (otherwise Python could deduce it). Let's compare it to an uninterruped custom date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a custom range\n",
    "date_range = pd.date_range(start=min(df.index), \n",
    "                           end=max(df.index), \n",
    "                           freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The difference in length between the custom date range and our dataset is {(len(date_range)-len(df))}:')\n",
    "print(date_range.difference(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindexing our dataset, then performing imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will append the previously missing datetimes, and create null values in our target variable\n",
    "df = df.reindex(date_range)\n",
    "\n",
    "# we fill in the blanks with values that lie on a linear curve between existing data points\n",
    "df['EnergyDemand_in_MW'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "# now we have a neatly continuous datetime index\n",
    "print(f'The df.index.freq is now: {df.index.freq}, indicating that we no longer have missing instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting up the date-timestamp column into its different components will allow us to find patterns for different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dow'] = df.index.dayofweek\n",
    "df['doy'] = df.index.dayofyear\n",
    "df['year'] = df.index.year\n",
    "df['month'] = df.index.month\n",
    "df['quarter'] = df.index.quarter\n",
    "df['hour'] = df.index.hour\n",
    "df['weekday'] = df.index.weekday_name\n",
    "df['woy'] = df.index.weekofyear\n",
    "df['dom'] = df.index.day # Day of Month\n",
    "df['date'] = df.index.date \n",
    "\n",
    "# Adding the season number\n",
    "df['season'] = df['month'].apply(lambda month_number: (month_number%12 + 3)//3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas_profiling to get an overview of our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix indicates that with strongest correlations among all other variables, \"dow\" (day of week) and \"hour\" will be interesting to look at in the context of predicting our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Visuals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the energy consumption over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since plotly doesn't allow us to access the index, let's copy it into a column \n",
    "df['date_and_time'] = df.index\n",
    "\n",
    "# plotting\n",
    "fig = px.line(df,\n",
    "              x='date_and_time',\n",
    "              y='EnergyDemand_in_MW',\n",
    "              title=f'Power Demand (MW) over time [{min(df.year)} - {max(df.year)}]')\n",
    "fig.update_traces(line=dict(width=0.05))\n",
    "fig.update_layout(xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely identify a seasonal pattern here, but there does not seem to be any immediately apparent trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date and Time Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our previously extracted date and time features to see if recurring patterns emerge from the aggregated data. Take for instance, the power demand throughout the day for each weekday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated data\n",
    "_ = df\\\n",
    "    .groupby(['hour', 'weekday'], as_index=False)\\\n",
    "    .agg({'EnergyDemand_in_MW':'median'})\n",
    "\n",
    "# plotting\n",
    "fig = px.line(_, \n",
    "              x='hour', \n",
    "              y='EnergyDemand_in_MW', \n",
    "              color='weekday', \n",
    "              title='Median Hourly Power Demand per Weekday')\n",
    "fig.update_layout(xaxis_title='Hour',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the deamand for electricity is lower during the weekends, and dips a little sooner on Friday afternoons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at power demand per season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated data\n",
    "_ = df\\\n",
    "    .groupby(['hour', 'season'], as_index=False)\\\n",
    "    .agg({'EnergyDemand_in_MW':'median'})\n",
    "\n",
    "# plotting\n",
    "fig = px.line(_,\n",
    "              x='hour', \n",
    "              y='EnergyDemand_in_MW', \n",
    "              color='season', \n",
    "              title='Median Hourly Power Demand per Season')\n",
    "fig.update_layout(xaxis_title='Hour',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, those air conditioners are turned up during the summer afternoons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposing the Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data points over time can have both a trend (upward or downward) and/or seasonality. As we have established in our EDA, these aspects seem to play a role in this dataset.\n",
    "\n",
    "Because the seasonal variation in our dataset appears constant over time as indicated by the repeating spikes with about the same level of increase and decrease in the \"Power Demand Over Time\" chart, we will use the additive model for decomposition (as opposed to the multiplicative model, which is useful for cases where seasonal variation increases over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# seasonal_decompose needs a dataframe with a datetime index\n",
    "series = df[['EnergyDemand_in_MW']]\n",
    "frequency = 24*365\n",
    "\n",
    "# decomposing the time-series, with the frequency being 24 hours per 365 days\n",
    "decomposed = seasonal_decompose(series, model='additive', freq=frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the different elements constituting our time-series\n",
    "def plot_decompositions(decompositions, titles, line_widths):\n",
    "    for d, t, lw in zip(decompositions, titles, line_widths):\n",
    "        \n",
    "        # draw a line plot of the data\n",
    "        fig = px.line(d,\n",
    "              y='EnergyDemand_in_MW',\n",
    "              title=t,\n",
    "              height=300)\n",
    "        \n",
    "        # adjust line width\n",
    "        fig.update_traces(line=dict(width=lw))\n",
    "        \n",
    "        # change layout of axes and the figure's margins \n",
    "        # to emulate tight_layout\n",
    "        fig.update_layout(\n",
    "            xaxis=dict(\n",
    "                showticklabels=False,\n",
    "                linewidth=1\n",
    "            ),\n",
    "            yaxis=dict(title=''),\n",
    "            margin=go.layout.Margin(\n",
    "                l=40, r=40, b=0, t=40, pad=0\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # display\n",
    "        fig.show()\n",
    "\n",
    "# calling the function \n",
    "plot_decompositions(decompositions=[decomposed.trend, \n",
    "                                    decomposed.seasonal, \n",
    "                                    decomposed.resid],\n",
    "                    titles=['Trend', \n",
    "                            'Seasonality',\n",
    "                            'Residuals'],\n",
    "                    line_widths=[2, 0.025, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the following methods:\n",
    "\n",
    "Triple Exponential Smoothing: Holt-Winter's\n",
    "Explicit Multi-Seasonality: Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test\n",
    "The goal is to accurately predict up to 12 month's worth of energy demand. We will restrict our training data to a couple of years leading up to that, to make sure we don't capture any outdated trends (industry shifts). This doesn't seem to be the case, judging from our prior visualisations, but it will keep our computational load low and allow us to iterate over models more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find out last date for which the data are available, so we can eventually compare the forecast with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The last date time point in our dataframe is: {max(df.index)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually setting the cutoff date\n",
    "CUTOFF_DATE = pd.to_datetime('2017-08-01')\n",
    "TIME_DELTA = pd.DateOffset(years=8)\n",
    "\n",
    "# splitting in training and testing datasets\n",
    "train = df.loc[(df.index < CUTOFF_DATE) & (df.index >= CUTOFF_DATE-TIME_DELTA) ].copy()\n",
    "test = df.loc[df.index >= CUTOFF_DATE].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Training shape: {train.shape} \\nTesting shape: {test.shape}\\n')\n",
    "print(f'The training set lies between the dates: {min(train.index)} and {max(train.index)}')\n",
    "print(f'For the testing set, the dates are: {min(test.index)} and {max(test.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the train and test datasets\n",
    "train.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/train.csv', index = False)\n",
    "test.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt-Winter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# exponential smoothing only takes into consideration patterns in the target variable\n",
    "# so we discard the other features\n",
    "exp_smooth_train, exp_smooth_test = train['EnergyDemand_in_MW'], test['EnergyDemand_in_MW']\n",
    "\n",
    "# fit & predict\n",
    "holt_winter = sm.tsa.ExponentialSmoothing(exp_smooth_train,\n",
    "                                          seasonal_periods=24*365,\n",
    "                                          seasonal='add').fit()\n",
    "y_hat_holt_winter = holt_winter.forecast(len(exp_smooth_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=exp_smooth_test.index, y=exp_smooth_test,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=y_hat_holt_winter.index, y=y_hat_holt_winter,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_layout(title='Holt-Winter Forecast of Hourly Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for an algorithm that only considers patterns in the history of the target variable!\n",
    "\n",
    "We can see it fall short around the winter holiday period until March and late during the summer, but it has clearly recognised the frequency and the degree of variance of the seasonal patterns. Let's quantify its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    \"\"\" Mean Absolute Percentage Error \"\"\"\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # take the percentage error\n",
    "    pe = (y_true - y_pred) / y_true\n",
    "    \n",
    "    # take the absolute values\n",
    "    ape = np.abs(pe)\n",
    "    \n",
    "    # quantify the performance in a single number\n",
    "    mape = np.mean(ape)\n",
    "    \n",
    "    return f'{mape*100:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_hw = mape(y_true=exp_smooth_test, y_pred=y_hat_holt_winter)\n",
    "print(f'Our Holt-Winter model has a mean average percentage error of {mape_hw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the Holt-Winter model in greater detail \n",
    "Let's look at some intra-day predictions at the beginning and the end of the requested forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length set to 92 days (approximately a fiscal quarter)\n",
    "interval = 24 * 92\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = exp_smooth_test.iloc[:interval].index, exp_smooth_test.iloc[:interval]\n",
    "x_pred, y_pred = y_hat_holt_winter.iloc[:interval].index, y_hat_holt_winter.iloc[:interval]\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Holt-Winter Intra-Day Forecast of First {interval} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the first {interval} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length set to 92 days (approximately a fiscal quarter)\n",
    "interval = -24 * 92\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = exp_smooth_test.iloc[interval:].index, exp_smooth_test.iloc[interval:]\n",
    "x_pred, y_pred = y_hat_holt_winter.iloc[interval:].index, y_hat_holt_winter.iloc[interval:]\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Holt-Winter Intra-Day Forecast of Last {abs(interval)} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the last {abs(interval)} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Issue of Double/Triple Seasonality\n",
    "The Holt-Winters method crams all the seasonalities it can find in a single variable. From the decomposition, we can also tell it mistakes some seasonality for trend. While it seems to be a decent predictor of the overal trends, its precision could be fine tuned. From the model, however, it is not immediately obvious how to do that.\n",
    "\n",
    "Other methods, like SARIMA (seasonal ARIMA) allow you to specify the interval of different lags at which seasonality occurs more precisely. You can expose these by looking at autocorrelation, and configure your model accordingly.\n",
    "\n",
    "Auto-Correlogram & Partial Auto-Correlogram\n",
    "Below plots distinctly expose the daily recurring element (each lag is an hour, with spikes at the 24, 48, and 72 marks). Intuitively, this should make sense: If you want to know how much energy is going to be used tomorrow, changes are high it will be highly correlated to today's levels barring external variables like weather. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use tra.diff()(differenced data), because this time series is unit root process.\n",
    "fig,ax = plt.subplots(2,1,figsize=(20,10))\n",
    "fig = sm.graphics.tsa.plot_acf( train['EnergyDemand_in_MW'].diff().dropna(), lags=72, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(train['EnergyDemand_in_MW'].diff().dropna(), lags=72, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we have already exposed the weekly and yearly seasonal element during our exploratory data analysis. Checking for autocorrelation at lags 168 (24 x 7, weekly) and 8760 (24 x 365, yearly), respectively, would require a lot of memory. Instead, let's look at a model explicitly designed to account for multiple seasonalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for prophet model using 'ds' and 'y'\n",
    "train_prophet = train[['EnergyDemand_in_MW']]\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={\n",
    "                        'index':'ds', \n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "\n",
    "test_prophet = test[['EnergyDemand_in_MW']]\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={\n",
    "                        'index':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "def is_spring(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.month >= 3) & (date.month <= 5)\n",
    "\n",
    "def is_summer(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.month >= 6) & (date.month <= 8)\n",
    "\n",
    "def is_autumn(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.month >= 9) & (date.month <= 11)\n",
    "\n",
    "def is_winter(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.month >= 12) | (date.month <= 2)\n",
    "\n",
    "def is_weekend(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return date.weekday_name in ('Saturday', 'Sunday')\n",
    "\n",
    "# adding to train set\n",
    "train_prophet['is_spring'] = train_prophet['ds'].apply(is_spring)\n",
    "train_prophet['is_summer'] = train_prophet['ds'].apply(is_summer)\n",
    "train_prophet['is_autumn'] = train_prophet['ds'].apply(is_autumn)\n",
    "train_prophet['is_winter'] = train_prophet['ds'].apply(is_winter)\n",
    "train_prophet['is_weekend'] = train_prophet['ds'].apply(is_weekend)\n",
    "train_prophet['is_weekday'] = ~train_prophet['ds'].apply(is_weekend)\n",
    "\n",
    "# adding to test set\n",
    "test_prophet['is_spring'] = test_prophet['ds'].apply(is_spring)\n",
    "test_prophet['is_summer'] = test_prophet['ds'].apply(is_summer)\n",
    "test_prophet['is_autumn'] = test_prophet['ds'].apply(is_autumn)\n",
    "test_prophet['is_winter'] = test_prophet['ds'].apply(is_winter)\n",
    "test_prophet['is_weekend'] = test_prophet['ds'].apply(is_weekend)\n",
    "test_prophet['is_weekday'] = ~test_prophet['ds'].apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiating the class with custom settings\n",
    "prophet = Prophet(\n",
    "     daily_seasonality=False,\n",
    "    weekly_seasonality=False,\n",
    "    yearly_seasonality=False\n",
    ")\n",
    "\n",
    "# custom seasonalities to account for conditional variance \n",
    "# (more extreme trends in extreme seasons)\n",
    "prophet.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "prophet.add_seasonality(name='weekly_spring', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_spring')\n",
    "prophet.add_seasonality(name='weekly_summer', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_summer')\n",
    "prophet.add_seasonality(name='weekly_autumn', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_autumn')\n",
    "prophet.add_seasonality(name='weekly_winter', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_winter')\n",
    "prophet.add_seasonality(name='daily_spring',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_spring')\n",
    "prophet.add_seasonality(name='daily_summer',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_summer')\n",
    "prophet.add_seasonality(name='daily_autumn',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_autumn')\n",
    "prophet.add_seasonality(name='daily_winter',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_winter')\n",
    "prophet.add_seasonality(name='daily_weekend',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_weekend')\n",
    "prophet.add_seasonality(name='daily_weekday',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_weekday')\n",
    "\n",
    "# fitting the model\n",
    "prophet.fit(train_prophet);\n",
    "\n",
    "# part of the dataframe on which we want to make predictions\n",
    "future = test_prophet.drop(['y'], axis=1)\n",
    "\n",
    "# predicting values\n",
    "forecast = prophet.predict(future)\n",
    "\n",
    "# see https://github.com/facebook/prophet/issues/999 for the matplotlib_converts()\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "# plotting the seasonality components found\n",
    "_ = prophet.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_prophet.ds, y=test_prophet.y,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=forecast.ds, y=forecast.yhat,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_layout(title='Prophet Forecast of Hourly Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for Prophet\\'s predictions: {mape(test_prophet.y, forecast.yhat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = 24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = test_prophet.iloc[:interval].ds, test_prophet.iloc[:interval].y\n",
    "x_pred, y_pred = forecast.iloc[:interval].ds, forecast.iloc[:interval].yhat\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Prophet Intra-Day Forecast of First {interval} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the first {interval} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = -24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = test_prophet.iloc[:interval].ds, test_prophet.iloc[:interval].y\n",
    "x_pred, y_pred = forecast.iloc[:interval].ds, forecast.iloc[:interval].yhat\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Prophet Intra-Day Forecast of last {abs(interval)} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the first {abs(interval)} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dataframe for each of the last 10 full years present in the data frame\n",
    "# 2008\n",
    "id_date = pd.to_datetime('2008-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2008 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2009\n",
    "id_date = pd.to_datetime('2009-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2009 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2010\n",
    "id_date = pd.to_datetime('2010-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2010 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2011\n",
    "id_date = pd.to_datetime('2011-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2011 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2012\n",
    "id_date = pd.to_datetime('2012-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2012 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2013\n",
    "id_date = pd.to_datetime('2013-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2013 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2014\n",
    "id_date = pd.to_datetime('2014-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2014 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2015\n",
    "id_date = pd.to_datetime('2015-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2015 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2016\n",
    "id_date = pd.to_datetime('2016-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2016 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()\n",
    "#2017\n",
    "id_date = pd.to_datetime('2017-01-01')\n",
    "year_incr = pd.DateOffset(years=1)\n",
    "df_2017 = df.loc[(df.index >= id_date) & (df.index < id_date + year_incr) ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the yearly dataframes for predictions and plots\n",
    "df_2008 = df_2008.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2008['is_spring'] = df_2008['ds'].apply(is_spring)\n",
    "df_2008['is_summer'] = df_2008['ds'].apply(is_summer)\n",
    "df_2008['is_autumn'] = df_2008['ds'].apply(is_autumn)\n",
    "df_2008['is_winter'] = df_2008['ds'].apply(is_winter)\n",
    "df_2008['is_weekend'] = df_2008['ds'].apply(is_weekend)\n",
    "df_2008['is_weekday'] = ~df_2008['ds'].apply(is_weekend)\n",
    "\n",
    "df_2009 = df_2009.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2009['is_spring'] = df_2009['ds'].apply(is_spring)\n",
    "df_2009['is_summer'] = df_2009['ds'].apply(is_summer)\n",
    "df_2009['is_autumn'] = df_2009['ds'].apply(is_autumn)\n",
    "df_2009['is_winter'] = df_2009['ds'].apply(is_winter)\n",
    "df_2009['is_weekend'] = df_2009['ds'].apply(is_weekend)\n",
    "df_2009['is_weekday'] = ~df_2009['ds'].apply(is_weekend)\n",
    "\n",
    "df_2010 = df_2010.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2010['is_spring'] = df_2010['ds'].apply(is_spring)\n",
    "df_2010['is_summer'] = df_2010['ds'].apply(is_summer)\n",
    "df_2010['is_autumn'] = df_2010['ds'].apply(is_autumn)\n",
    "df_2010['is_winter'] = df_2010['ds'].apply(is_winter)\n",
    "df_2010['is_weekend'] = df_2010['ds'].apply(is_weekend)\n",
    "df_2010['is_weekday'] = ~df_2010['ds'].apply(is_weekend)\n",
    "\n",
    "df_2011 = df_2011.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2011['is_spring'] = df_2011['ds'].apply(is_spring)\n",
    "df_2011['is_summer'] = df_2011['ds'].apply(is_summer)\n",
    "df_2011['is_autumn'] = df_2011['ds'].apply(is_autumn)\n",
    "df_2011['is_winter'] = df_2011['ds'].apply(is_winter)\n",
    "df_2011['is_weekend'] = df_2011['ds'].apply(is_weekend)\n",
    "df_2011['is_weekday'] = ~df_2011['ds'].apply(is_weekend)\n",
    "\n",
    "df_2012 = df_2012.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2012['is_spring'] = df_2012['ds'].apply(is_spring)\n",
    "df_2012['is_summer'] = df_2012['ds'].apply(is_summer)\n",
    "df_2012['is_autumn'] = df_2012['ds'].apply(is_autumn)\n",
    "df_2012['is_winter'] = df_2012['ds'].apply(is_winter)\n",
    "df_2012['is_weekend'] = df_2012['ds'].apply(is_weekend)\n",
    "df_2012['is_weekday'] = ~df_2012['ds'].apply(is_weekend)\n",
    "\n",
    "df_2013 = df_2013.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2013['is_spring'] = df_2013['ds'].apply(is_spring)\n",
    "df_2013['is_summer'] = df_2013['ds'].apply(is_summer)\n",
    "df_2013['is_autumn'] = df_2013['ds'].apply(is_autumn)\n",
    "df_2013['is_winter'] = df_2013['ds'].apply(is_winter)\n",
    "df_2013['is_weekend'] = df_2013['ds'].apply(is_weekend)\n",
    "df_2013['is_weekday'] = ~df_2013['ds'].apply(is_weekend)\n",
    "\n",
    "df_2014 = df_2014.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2014['is_spring'] = df_2014['ds'].apply(is_spring)\n",
    "df_2014['is_summer'] = df_2014['ds'].apply(is_summer)\n",
    "df_2014['is_autumn'] = df_2014['ds'].apply(is_autumn)\n",
    "df_2014['is_winter'] = df_2014['ds'].apply(is_winter)\n",
    "df_2014['is_weekend'] = df_2014['ds'].apply(is_weekend)\n",
    "df_2014['is_weekday'] = ~df_2014['ds'].apply(is_weekend)\n",
    "\n",
    "df_2015 = df_2015.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2015['is_spring'] = df_2015['ds'].apply(is_spring)\n",
    "df_2015['is_summer'] = df_2015['ds'].apply(is_summer)\n",
    "df_2015['is_autumn'] = df_2015['ds'].apply(is_autumn)\n",
    "df_2015['is_winter'] = df_2015['ds'].apply(is_winter)\n",
    "df_2015['is_weekend'] = df_2015['ds'].apply(is_weekend)\n",
    "df_2015['is_weekday'] = ~df_2015['ds'].apply(is_weekend)\n",
    "\n",
    "df_2016 = df_2016.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2016['is_spring'] = df_2016['ds'].apply(is_spring)\n",
    "df_2016['is_summer'] = df_2016['ds'].apply(is_summer)\n",
    "df_2016['is_autumn'] = df_2016['ds'].apply(is_autumn)\n",
    "df_2016['is_winter'] = df_2016['ds'].apply(is_winter)\n",
    "df_2016['is_weekend'] = df_2016['ds'].apply(is_weekend)\n",
    "df_2016['is_weekday'] = ~df_2016['ds'].apply(is_weekend)\n",
    "\n",
    "df_2017 = df_2017.rename(columns={\n",
    "                        'date_and_time':'ds',\n",
    "                        'EnergyDemand_in_MW':'y'\n",
    "                    })\n",
    "df_2017['is_spring'] = df_2017['ds'].apply(is_spring)\n",
    "df_2017['is_summer'] = df_2017['ds'].apply(is_summer)\n",
    "df_2017['is_autumn'] = df_2017['ds'].apply(is_autumn)\n",
    "df_2017['is_winter'] = df_2017['ds'].apply(is_winter)\n",
    "df_2017['is_weekend'] = df_2017['ds'].apply(is_weekend)\n",
    "df_2017['is_weekday'] = ~df_2017['ds'].apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasting y in each yearly dataframe using the fitted model\n",
    "df_forecast_2008 = prophet.predict(df_2008)\n",
    "df_forecast_2009 = prophet.predict(df_2009)\n",
    "df_forecast_2010 = prophet.predict(df_2010)\n",
    "df_forecast_2011 = prophet.predict(df_2011)\n",
    "df_forecast_2012 = prophet.predict(df_2012)\n",
    "df_forecast_2013 = prophet.predict(df_2013)\n",
    "df_forecast_2014 = prophet.predict(df_2014)\n",
    "df_forecast_2015 = prophet.predict(df_2015)\n",
    "df_forecast_2016 = prophet.predict(df_2016)\n",
    "df_forecast_2017 = prophet.predict(df_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the year from the ds column for each yearly dataframe\n",
    "df_2008['ds'] = df_2008['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2009['ds'] = df_2009['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2010['ds'] = df_2010['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2011['ds'] = df_2011['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2012['ds'] = df_2012['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2013['ds'] = df_2013['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2014['ds'] = df_2014['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2015['ds'] = df_2015['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2016['ds'] = df_2016['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_2017['ds'] = df_2017['ds'].dt.strftime('%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_year = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the year from the ds column for each yearly forecast dataframe to prepare it for the GUI\n",
    "df_forecast_2008['ds'] = df_forecast_2008['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2009['ds'] = df_forecast_2009['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2010['ds'] = df_forecast_2010['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2011['ds'] = df_forecast_2011['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2012['ds'] = df_forecast_2012['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2013['ds'] = df_forecast_2013['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2014['ds'] = df_forecast_2014['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2015['ds'] = df_forecast_2015['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2016['ds'] = df_forecast_2016['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2017['ds'] = df_forecast_2017['ds'].dt.strftime('%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_2010.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the dataframe for the five year forecast for the GUI starting from 2020\n",
    "future_data = prophet.make_future_dataframe(periods=77490, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data['is_spring'] = future_data['ds'].apply(is_spring)\n",
    "future_data['is_summer'] = future_data['ds'].apply(is_summer)\n",
    "future_data['is_autumn'] = future_data['ds'].apply(is_autumn)\n",
    "future_data['is_winter'] = future_data['ds'].apply(is_winter)\n",
    "future_data['is_weekend'] = future_data['ds'].apply(is_weekend)\n",
    "future_data['is_weekday'] = ~future_data['ds'].apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data = prophet.predict(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet.plot(forecast_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data frames for use in the GUI\n",
    "id_date_2020 = pd.to_datetime('2020-01-01 00:00:00')\n",
    "df_forecast_2020 = forecast_data.loc[(forecast_data.ds >= id_date_2020) & (forecast_data.ds < id_date_2020+year_incr) ].copy()\n",
    "id_date_2021 = pd.to_datetime('2021-01-01 00:00:00')\n",
    "df_forecast_2021 = forecast_data.loc[(forecast_data.ds >= id_date_2021) & (forecast_data.ds < id_date_2021+year_incr) ].copy()\n",
    "id_date_2022 = pd.to_datetime('2022-01-01 00:00:00')\n",
    "df_forecast_2022 = forecast_data.loc[(forecast_data.ds >= id_date_2022) & (forecast_data.ds < id_date_2022+year_incr) ].copy()\n",
    "id_date_2023 = pd.to_datetime('2023-01-01 00:00:00')\n",
    "df_forecast_2023 = forecast_data.loc[(forecast_data.ds >= id_date_2023) & (forecast_data.ds < id_date_2023+year_incr) ].copy()\n",
    "id_date_2024 = pd.to_datetime('2024-01-01 00:00:00')\n",
    "df_forecast_2024 = forecast_data.loc[(forecast_data.ds >= id_date_2024) & (forecast_data.ds < id_date_2024+year_incr) ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_2020.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing yearly forecast data frames for the use in GUI\n",
    "df_forecast_2020['ds'] = df_forecast_2020['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2021['ds'] = df_forecast_2021['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2022['ds'] = df_forecast_2022['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2023['ds'] = df_forecast_2023['ds'].dt.strftime('%m-%d %H:%M:%S')\n",
    "df_forecast_2024['ds'] = df_forecast_2024['ds'].dt.strftime('%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_forecast_2020.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving data frames\n",
    "# Yearly actual data\n",
    "df_with_year.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_with_year.csv', index = False)\n",
    "df_2008.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2008.csv', index = False)\n",
    "df_2009.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2009.csv', index = False)\n",
    "df_2010.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2010.csv', index = False)\n",
    "df_2011.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2011.csv', index = False)\n",
    "df_2012.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2012.csv', index = False)\n",
    "df_2013.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2013.csv', index = False)\n",
    "df_2014.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2014.csv', index = False)\n",
    "df_2015.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2015.csv', index = False)\n",
    "df_2016.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2016.csv', index = False)\n",
    "df_2017.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_2017.csv', index = False)\n",
    "#Yearly forecasted data\n",
    "df_forecast_2008.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2008.csv', index = False)\n",
    "df_forecast_2009.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2009.csv', index = False)\n",
    "df_forecast_2010.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2010.csv', index = False)\n",
    "df_forecast_2011.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2011.csv', index = False)\n",
    "df_forecast_2012.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2012.csv', index = False)\n",
    "df_forecast_2013.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2013.csv', index = False)\n",
    "df_forecast_2014.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2014.csv', index = False)\n",
    "df_forecast_2015.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2015.csv', index = False)\n",
    "df_forecast_2016.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2016.csv', index = False)\n",
    "df_forecast_2017.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2017.csv', index = False)\n",
    "df_forecast_2020.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2020.csv', index = False)\n",
    "df_forecast_2021.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2021.csv', index = False)\n",
    "df_forecast_2022.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2022.csv', index = False)\n",
    "df_forecast_2023.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2023.csv', index = False)\n",
    "df_forecast_2024.to_csv('/home/jovyan/Capstone_Data_Files/Capstone_DataFrames/df_forecast_2024.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
